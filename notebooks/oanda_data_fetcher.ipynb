{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# OANDA REST-V20 API Data Fetcher\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. List all available instruments from OANDA REST-V20 API\n",
    "2. Download historical candle data for specified instruments from 2000 to now\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before running this notebook:\n",
    "1. Copy `.env.example` to `.env`\n",
    "2. Fill in your OANDA API token and account ID in the `.env` file\n",
    "3. Install requirements: `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all prices of EUR_CHF [M1] from 2019-01-01 21:00:00+00:00 to 2019-01-02 03:00:00+00:00 ... got 246 rows.\n",
      "Fetching all prices of EUR_CHF [M1] from 2019-01-02 03:00:00+00:00 to 2019-01-02 04:00:00+00:00 ... got 44 rows.\n",
      "                               m_o      m_h      m_l      m_c  m_t      b_o  \\\n",
      "time                                                                          \n",
      "2019-01-01 22:00:00+00:00  1.12566  1.12566  1.12566  1.12566    1  1.12511   \n",
      "2019-01-01 22:01:00+00:00  1.12582  1.12582  1.12582  1.12582    1  1.12528   \n",
      "2019-01-01 22:02:00+00:00  1.12586  1.12586  1.12586  1.12586    1  1.12532   \n",
      "2019-01-01 22:03:00+00:00  1.12582  1.12582  1.12564  1.12564    4  1.12528   \n",
      "2019-01-01 22:06:00+00:00  1.12558  1.12570  1.12558  1.12570    3  1.12504   \n",
      "...                            ...      ...      ...      ...  ...      ...   \n",
      "2019-01-02 03:54:00+00:00  1.12584  1.12586  1.12584  1.12584    3  1.12572   \n",
      "2019-01-02 03:55:00+00:00  1.12586  1.12586  1.12584  1.12584    2  1.12575   \n",
      "2019-01-02 03:56:00+00:00  1.12587  1.12587  1.12587  1.12587    1  1.12576   \n",
      "2019-01-02 03:58:00+00:00  1.12585  1.12585  1.12585  1.12585    1  1.12573   \n",
      "2019-01-02 03:59:00+00:00  1.12587  1.12587  1.12587  1.12587    1  1.12575   \n",
      "\n",
      "                               b_h      b_l      b_c  b_t      a_o      a_h  \\\n",
      "time                                                                          \n",
      "2019-01-01 22:00:00+00:00  1.12511  1.12511  1.12511    1  1.12620  1.12620   \n",
      "2019-01-01 22:01:00+00:00  1.12528  1.12528  1.12528    1  1.12637  1.12637   \n",
      "2019-01-01 22:02:00+00:00  1.12532  1.12532  1.12532    1  1.12641  1.12641   \n",
      "2019-01-01 22:03:00+00:00  1.12528  1.12510  1.12510    4  1.12637  1.12637   \n",
      "2019-01-01 22:06:00+00:00  1.12516  1.12504  1.12516    3  1.12613  1.12625   \n",
      "...                            ...      ...      ...  ...      ...      ...   \n",
      "2019-01-02 03:54:00+00:00  1.12575  1.12572  1.12572    3  1.12597  1.12598   \n",
      "2019-01-02 03:55:00+00:00  1.12575  1.12574  1.12574    2  1.12598  1.12598   \n",
      "2019-01-02 03:56:00+00:00  1.12576  1.12576  1.12576    1  1.12598  1.12598   \n",
      "2019-01-02 03:58:00+00:00  1.12573  1.12573  1.12573    1  1.12597  1.12597   \n",
      "2019-01-02 03:59:00+00:00  1.12575  1.12575  1.12575    1  1.12599  1.12599   \n",
      "\n",
      "                               a_l      a_c  a_t  \n",
      "time                                              \n",
      "2019-01-01 22:00:00+00:00  1.12620  1.12620    1  \n",
      "2019-01-01 22:01:00+00:00  1.12637  1.12637    1  \n",
      "2019-01-01 22:02:00+00:00  1.12641  1.12641    1  \n",
      "2019-01-01 22:03:00+00:00  1.12619  1.12619    4  \n",
      "2019-01-01 22:06:00+00:00  1.12613  1.12625    3  \n",
      "...                            ...      ...  ...  \n",
      "2019-01-02 03:54:00+00:00  1.12597  1.12597    3  \n",
      "2019-01-02 03:55:00+00:00  1.12595  1.12595    2  \n",
      "2019-01-02 03:56:00+00:00  1.12598  1.12598    1  \n",
      "2019-01-02 03:58:00+00:00  1.12597  1.12597    1  \n",
      "2019-01-02 03:59:00+00:00  1.12599  1.12599    1  \n",
      "\n",
      "[290 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "import requests\n",
    "\n",
    "%run ../scripts/fetch_data.py\n",
    "\n",
    "# get_candles_all_prices('EUR_CHF', start_date='2009-01-01', end_date='2009-12-31', granularity='D') # Checked - daily works.\n",
    "start_time = datetime(2019, 1, 1, 21, 0, tzinfo=timezone.utc)\n",
    "end_time   = datetime(2019, 1, 2, 4, 0, tzinfo=timezone.utc)\n",
    "\n",
    "# Fetch candles for the specified time range\n",
    "# candles = _oanda_candles_request(requests.Session(), 'EUR_CHF', 'M1', start_time, end_time, price_type='M', count=None)\n",
    "candles = get_candles_all_prices_during_interval('EUR_CHF', start_time, end_time, granularity='M1')\n",
    "print(candles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4943c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big file loaded.\n",
      "NaNs dropped.\n",
      "Processed data saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "big_file = '../data/processed/merged_data.parquet'\n",
    "df = pd.read_parquet(big_file)\n",
    "print(\"Big file loaded.\")\n",
    "y_col = 'm_realized_vol_5min_future'\n",
    "# Drop rows with NaN in the target column - assume they are at the beginning of the dataset:\n",
    "# find the first valid index\n",
    "first_valid_index = df[y_col].first_valid_index()\n",
    "df = df.loc[first_valid_index:]\n",
    "print(\"NaNs dropped.\")\n",
    "df.to_parquet(big_file)\n",
    "print(\"Processed data saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Function: List All Available Instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_instruments():\n",
    "    \"\"\"\n",
    "    List all instruments available in the OANDA account.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing instrument details\n",
    "    \"\"\"\n",
    "    url = f\"{API_URL}/v3/accounts/{ACCOUNT_ID}/instruments\"\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {API_TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        instruments = data.get('instruments', [])\n",
    "        \n",
    "        # Create a DataFrame with relevant information\n",
    "        instruments_data = []\n",
    "        for inst in instruments:\n",
    "            instruments_data.append({\n",
    "                'name': inst.get('name'),\n",
    "                'type': inst.get('type'),\n",
    "                'displayName': inst.get('displayName'),\n",
    "                'pipLocation': inst.get('pipLocation'),\n",
    "                'displayPrecision': inst.get('displayPrecision'),\n",
    "                'tradeUnitsPrecision': inst.get('tradeUnitsPrecision'),\n",
    "                'minimumTradeSize': inst.get('minimumTradeSize'),\n",
    "                'maximumTrailingStopDistance': inst.get('maximumTrailingStopDistance'),\n",
    "                'minimumTrailingStopDistance': inst.get('minimumTrailingStopDistance')\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(instruments_data)\n",
    "        print(f\"Found {len(df)} instruments\")\n",
    "        return df\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching instruments: {e}\")\n",
    "        if hasattr(e.response, 'text'):\n",
    "            print(f\"Response: {e.response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Test: List All Instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68 instruments\n",
      "\n",
      "First 10 instruments:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "displayName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pipLocation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "displayPrecision",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tradeUnitsPrecision",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "minimumTradeSize",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "maximumTrailingStopDistance",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "minimumTrailingStopDistance",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e90e88d7-92f8-4650-aa46-6c60b44858a3",
       "rows": [
        [
         "0",
         "TRY_JPY",
         "CURRENCY",
         "TRY/JPY",
         "-2",
         "3",
         "0",
         "1",
         "100.000",
         "0.050"
        ],
        [
         "1",
         "AUD_JPY",
         "CURRENCY",
         "AUD/JPY",
         "-2",
         "3",
         "0",
         "1",
         "100.000",
         "0.050"
        ],
        [
         "2",
         "USD_CNH",
         "CURRENCY",
         "USD/CNH",
         "-4",
         "5",
         "0",
         "1",
         "1.00000",
         "0.00050"
        ],
        [
         "3",
         "NZD_JPY",
         "CURRENCY",
         "NZD/JPY",
         "-2",
         "3",
         "0",
         "1",
         "100.000",
         "0.050"
        ],
        [
         "4",
         "EUR_GBP",
         "CURRENCY",
         "EUR/GBP",
         "-4",
         "5",
         "0",
         "1",
         "1.00000",
         "0.00050"
        ],
        [
         "5",
         "CHF_HKD",
         "CURRENCY",
         "CHF/HKD",
         "-4",
         "5",
         "0",
         "1",
         "1.00000",
         "0.00050"
        ],
        [
         "6",
         "USD_CZK",
         "CURRENCY",
         "USD/CZK",
         "-4",
         "5",
         "0",
         "1",
         "1.00000",
         "0.00050"
        ],
        [
         "7",
         "NZD_HKD",
         "CURRENCY",
         "NZD/HKD",
         "-4",
         "5",
         "0",
         "1",
         "1.00000",
         "0.00050"
        ],
        [
         "8",
         "EUR_NOK",
         "CURRENCY",
         "EUR/NOK",
         "-4",
         "5",
         "0",
         "1",
         "1.00000",
         "0.00050"
        ],
        [
         "9",
         "USD_CAD",
         "CURRENCY",
         "USD/CAD",
         "-4",
         "5",
         "0",
         "1",
         "1.00000",
         "0.00050"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>displayName</th>\n",
       "      <th>pipLocation</th>\n",
       "      <th>displayPrecision</th>\n",
       "      <th>tradeUnitsPrecision</th>\n",
       "      <th>minimumTradeSize</th>\n",
       "      <th>maximumTrailingStopDistance</th>\n",
       "      <th>minimumTrailingStopDistance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRY_JPY</td>\n",
       "      <td>CURRENCY</td>\n",
       "      <td>TRY/JPY</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUD_JPY</td>\n",
       "      <td>CURRENCY</td>\n",
       "      <td>AUD/JPY</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USD_CNH</td>\n",
       "      <td>CURRENCY</td>\n",
       "      <td>USD/CNH</td>\n",
       "      <td>-4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NZD_JPY</td>\n",
       "      <td>CURRENCY</td>\n",
       "      <td>NZD/JPY</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EUR_GBP</td>\n",
       "      <td>CURRENCY</td>\n",
       "      <td>EUR/GBP</td>\n",
       "      <td>-4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CHF_HKD</td>\n",
       "      <td>CURRENCY</td>\n",
       "      <td>CHF/HKD</td>\n",
       "      <td>-4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>USD_CZK</td>\n",
       "      <td>CURRENCY</td>\n",
       "      <td>USD/CZK</td>\n",
       "      <td>-4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NZD_HKD</td>\n",
       "      <td>CURRENCY</td>\n",
       "      <td>NZD/HKD</td>\n",
       "      <td>-4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EUR_NOK</td>\n",
       "      <td>CURRENCY</td>\n",
       "      <td>EUR/NOK</td>\n",
       "      <td>-4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>USD_CAD</td>\n",
       "      <td>CURRENCY</td>\n",
       "      <td>USD/CAD</td>\n",
       "      <td>-4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name      type displayName  pipLocation  displayPrecision  \\\n",
       "0  TRY_JPY  CURRENCY     TRY/JPY           -2                 3   \n",
       "1  AUD_JPY  CURRENCY     AUD/JPY           -2                 3   \n",
       "2  USD_CNH  CURRENCY     USD/CNH           -4                 5   \n",
       "3  NZD_JPY  CURRENCY     NZD/JPY           -2                 3   \n",
       "4  EUR_GBP  CURRENCY     EUR/GBP           -4                 5   \n",
       "5  CHF_HKD  CURRENCY     CHF/HKD           -4                 5   \n",
       "6  USD_CZK  CURRENCY     USD/CZK           -4                 5   \n",
       "7  NZD_HKD  CURRENCY     NZD/HKD           -4                 5   \n",
       "8  EUR_NOK  CURRENCY     EUR/NOK           -4                 5   \n",
       "9  USD_CAD  CURRENCY     USD/CAD           -4                 5   \n",
       "\n",
       "   tradeUnitsPrecision minimumTradeSize maximumTrailingStopDistance  \\\n",
       "0                    0                1                     100.000   \n",
       "1                    0                1                     100.000   \n",
       "2                    0                1                     1.00000   \n",
       "3                    0                1                     100.000   \n",
       "4                    0                1                     1.00000   \n",
       "5                    0                1                     1.00000   \n",
       "6                    0                1                     1.00000   \n",
       "7                    0                1                     1.00000   \n",
       "8                    0                1                     1.00000   \n",
       "9                    0                1                     1.00000   \n",
       "\n",
       "  minimumTrailingStopDistance  \n",
       "0                       0.050  \n",
       "1                       0.050  \n",
       "2                     0.00050  \n",
       "3                       0.050  \n",
       "4                     0.00050  \n",
       "5                     0.00050  \n",
       "6                     0.00050  \n",
       "7                     0.00050  \n",
       "8                     0.00050  \n",
       "9                     0.00050  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instrument types available:\n",
      "type\n",
      "CURRENCY    68\n",
      "Name: count, dtype: int64\n",
      "\n",
      "All instrument names:\n",
      "['TRY_JPY', 'AUD_JPY', 'USD_CNH', 'NZD_JPY', 'EUR_GBP', 'CHF_HKD', 'USD_CZK', 'NZD_HKD', 'EUR_NOK', 'USD_CAD', 'EUR_AUD', 'EUR_SGD', 'USD_HKD', 'CAD_HKD', 'USD_CHF', 'AUD_HKD', 'NZD_CHF', 'AUD_CHF', 'GBP_CHF', 'USD_THB', 'EUR_HKD', 'CHF_JPY', 'GBP_HKD', 'EUR_NZD', 'AUD_SGD', 'EUR_JPY', 'EUR_TRY', 'USD_JPY', 'SGD_JPY', 'GBP_ZAR', 'ZAR_JPY', 'USD_SEK', 'GBP_SGD', 'CAD_CHF', 'AUD_NZD', 'HKD_JPY', 'USD_NOK', 'GBP_AUD', 'USD_PLN', 'EUR_ZAR', 'NZD_USD', 'USD_ZAR', 'CAD_JPY', 'CAD_SGD', 'USD_HUF', 'EUR_CAD', 'CHF_ZAR', 'USD_DKK', 'EUR_HUF', 'EUR_CHF', 'EUR_DKK', 'EUR_USD', 'EUR_CZK', 'NZD_CAD', 'SGD_CHF', 'GBP_JPY', 'USD_TRY', 'GBP_PLN', 'AUD_USD', 'GBP_USD', 'USD_MXN', 'GBP_CAD', 'AUD_CAD', 'EUR_PLN', 'GBP_NZD', 'EUR_SEK', 'USD_SGD', 'NZD_SGD']\n"
     ]
    }
   ],
   "source": [
    "# Get all available instruments\n",
    "instruments_df = list_instruments()\n",
    "\n",
    "if instruments_df is not None:\n",
    "    # Display first few instruments\n",
    "    print(\"\\nFirst 10 instruments:\")\n",
    "    display(instruments_df.head(10))\n",
    "    \n",
    "    # Display instrument types\n",
    "    print(\"\\nInstrument types available:\")\n",
    "    print(instruments_df['type'].value_counts())\n",
    "    \n",
    "    # Display all instrument names\n",
    "    print(\"\\nAll instrument names:\")\n",
    "    print(instruments_df['name'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Function: Download Candle Data\n",
    "\n",
    "Downloads historical candle data for specified instruments from 2000 to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59592bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from functools import wraps\n",
    "\n",
    "def profile(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        profiler = cProfile.Profile()\n",
    "        profiler.enable()\n",
    "        result = func(*args, **kwargs)\n",
    "        profiler.disable()\n",
    "        stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "        print(f\"\\nProfiling results for {func.__name__}:\")\n",
    "        stats.print_stats(20)  # print top 20 lines\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# To profile a function, add @profile above its definition, e.g.:\n",
    "# @profile\n",
    "# def get_candles(...):\n",
    "#     ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OANDA_MAX_FREQ = 100 # Max 100 requests per second\n",
    "\n",
    "def _ensure_utc(dt):\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parquet_filename(instrument, year, granularity, save_dir=None):\n",
    "    fname = f\"{instrument}_{year}{'_' + granularity if granularity != 'D' else ''}.parquet\"\n",
    "    if save_dir:\n",
    "        fname = os.path.join(save_dir, fname)\n",
    "    return fname\n",
    "\n",
    "def _log_oanda_request(instrument, granularity, price_type, from_time, to_time, status_code, url, params):\n",
    "    import datetime\n",
    "    log_path = \"oanda_requests.log\"\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
    "    log_line = f\"[{timestamp}] [{instrument}] [{granularity}] [{price_type}] [{from_time}] [{to_time}] [{status_code}] [{url}] [{params}]\\n\"\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(log_line)\n",
    "\n",
    "def _oanda_candles_request(session, instrument, granularity, current_start, end_dt, price_type='M', count=5000):\n",
    "    \"\"\"Low-level single request wrapper. Returns (candles, status_code, error_message).\"\"\"\n",
    "    url = f\"{API_URL}/v3/instruments/{instrument}/candles\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {API_TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    params = {\n",
    "        'from': current_start,\n",
    "        'granularity': granularity,\n",
    "        'price': price_type,\n",
    "        'count': count\n",
    "    }\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, params=params)\n",
    "        status_code = response.status_code\n",
    "        _log_oanda_request(instrument, granularity, price_type, current_start, end_dt.isoformat(), status_code, url, params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        candles = data.get('candles', [])\n",
    "        return candles, status_code, None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        status_code = getattr(e.response, 'status_code', 'ERR') if hasattr(e, 'response') and e.response is not None else 'ERR'\n",
    "        _log_oanda_request(instrument, granularity, price_type, current_start, end_dt.isoformat(), status_code, url, params)\n",
    "        error_message = str(e)\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            error_message += f\"\\nResponse: {e.response.text}\"\n",
    "        return None, status_code, error_message\n",
    "\n",
    "def _transform_candles(candles, price_type):\n",
    "    rows = []\n",
    "    for c in candles:\n",
    "        base = {\n",
    "            'time': c['time'], \n",
    "        }\n",
    "        prefix, key = {'M': ('m', 'mid'), 'B': ('b', 'bid'), 'A': ('a', 'ask')}[price_type]\n",
    "        base.update({ # 'm_o','m_h','m_l','m_c', etc.\n",
    "            f'{prefix}_o': np.float32(c[key]['o']),\n",
    "            f'{prefix}_h': np.float32(c[key]['h']),\n",
    "            f'{prefix}_l': np.float32(c[key]['l']),\n",
    "            f'{prefix}_c': np.float32(c[key]['c']),\n",
    "            f'{prefix}_t': np.int32(c['volume'])\n",
    "        })\n",
    "        rows.append(base)\n",
    "    if not rows:\n",
    "        return None\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['time'] = pd.to_datetime(df['time'], utc=True)\n",
    "    df.set_index('time', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def _fetch_price_type_paginated(instrument, start_dt, end_dt, granularity, price_type):\n",
    "    \"\"\"Generic pagination for a single price type (M,B,A).\"\"\"\n",
    "    start_dt = _ensure_utc(start_dt); end_dt = _ensure_utc(end_dt)\n",
    "    all_parts = []\n",
    "    current_start = start_dt.isoformat()\n",
    "    with requests.Session() as session:\n",
    "        while True:\n",
    "            candles, status_code, error_message = _oanda_candles_request(session, instrument, granularity, current_start, end_dt, price_type=price_type)\n",
    "            if candles is None:\n",
    "                if error_message:\n",
    "                    print(f\"Error fetching {price_type} candles for {instrument}: {error_message}\")\n",
    "                break\n",
    "            if not candles:\n",
    "                break\n",
    "            part = _transform_candles(candles, price_type)\n",
    "            if part is not None:\n",
    "                all_parts.append(part)\n",
    "            last_candle_time = max(c['time'] for c in candles)\n",
    "            last_candle_dt = pd.to_datetime(last_candle_time, utc=True)\n",
    "            if last_candle_dt > end_dt or len(candles) < 5000:\n",
    "                break\n",
    "            current_start = last_candle_time\n",
    "            time.sleep(1.0 / OANDA_MAX_FREQ)\n",
    "    if not all_parts:\n",
    "        return None\n",
    "    df = pd.concat(all_parts)\n",
    "    df = df[(df.index >= start_dt) & (df.index <= end_dt)]\n",
    "    return df\n",
    "\n",
    "def fetch_candles(instrument, start_dt, end_dt, granularity, price_types=('M',)):\n",
    "    \"\"\"High-level fetch for one or multiple price types; returns merged DataFrame.\"\"\"\n",
    "    frames = []\n",
    "    for pt in price_types:\n",
    "        f = _fetch_price_type_paginated(instrument, start_dt, end_dt, granularity, pt)\n",
    "        if f is not None:\n",
    "            frames.append(f)\n",
    "    if not frames:\n",
    "        return None\n",
    "    merged = frames[0]\n",
    "    for add in frames[1:]:\n",
    "        merged = merged.join(add, how='outer')\n",
    "    merged.sort_index(inplace=True)\n",
    "    return merged\n",
    "\n",
    "def get_candles_all_prices(instrument, start_date='2005-01-01', end_date=None, granularity='S5', save_dir=None):\n",
    "    start_dt = _ensure_utc(datetime.strptime(start_date, '%Y-%m-%d'))\n",
    "    end_dt = _ensure_utc(datetime.strptime(end_date, '%Y-%m-%d')) if end_date else datetime.now(timezone.utc)\n",
    "    if save_dir: os.makedirs(save_dir, exist_ok=True)\n",
    "    for year in range(start_dt.year, end_dt.year + 1):\n",
    "        year_start = datetime(year, 1, 1, tzinfo=timezone.utc)\n",
    "        year_end = datetime(year, 12, 31, 23, 59, 59, tzinfo=timezone.utc)\n",
    "        fetch_start = max(start_dt, year_start)\n",
    "        fetch_end = min(end_dt, year_end)\n",
    "        df = fetch_candles(instrument, fetch_start, fetch_end, granularity, price_types=('M','B','A'))\n",
    "        if df is not None:\n",
    "            fname = _parquet_filename(instrument, year, granularity + '_BA', save_dir)\n",
    "            df.to_parquet(fname)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Test: Download Candle Data for Single Instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Download data for EUR_CHF\n",
    "# get_candles_all_prices('EUR_CHF', start_date='2013-01-01', end_date='2013-12-31', granularity='S5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Test: Load data back from a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample parquet rows: 3020635\n",
      "                           m_t  since\n",
      "time                                 \n",
      "2009-01-01 18:17:15+00:00    1    0.0\n",
      "2009-01-01 18:27:35+00:00    1  620.0\n",
      "2009-01-01 18:28:15+00:00    1   40.0\n",
      "2009-01-01 18:28:25+00:00    1   10.0\n",
      "2009-01-01 18:41:00+00:00    2  755.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity load test after refactor\n",
    "import pandas as pd\n",
    "try:\n",
    "    sample = pd.read_parquet('data/raw/EUR_CHF_2009_S5_BA.parquet')\n",
    "    print('Loaded sample parquet rows:', len(sample))\n",
    "    sample = sample[:5]\n",
    "    all_columns = sample.columns.tolist()\n",
    "    sample.drop(columns=[c for c in all_columns if c != 'm_t'], inplace=True)\n",
    "    sample['since'] = sample.index.to_series().diff().dt.total_seconds().fillna(0) # Good, but would require completing with arithmetic sequence, once the index is completed.\n",
    "    print(sample.head())\n",
    "except Exception as e:\n",
    "    print('Parquet load sanity check failed:', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acb405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 11 parquet files for missing data...\n",
      "Missing data found in file EUR_CHF_2005_S5_BA.parquet: percentage of missing values: 22.47%\n",
      "No missing data in file EUR_CHF_2006_S5_BA.parquet.\n",
      "No missing data in file EUR_CHF_2007_S5_BA.parquet.\n",
      "Missing data found in file EUR_CHF_2008_S5_BA.parquet: percentage of missing values: 0.58%\n",
      "No missing data in file EUR_CHF_2009_S5_BA.parquet.\n",
      "No missing data in file EUR_CHF_2010_S5_BA.parquet.\n",
      "No missing data in file EUR_CHF_2011_S5_BA.parquet.\n",
      "No missing data in file EUR_CHF_2012_S5_BA.parquet.\n",
      "No missing data in file EUR_CHF_2013_S5_BA.parquet.\n",
      "No missing data in file EUR_CHF_2014_S5_BA.parquet.\n",
      "No missing data in file EUR_CHF_2015_S5_BA.parquet.\n"
     ]
    }
   ],
   "source": [
    "#Question: are there missing data points in the downloaded data?\n",
    "import pandas as pd\n",
    "# Let's check all the data we have for NaNs\n",
    "import os\n",
    "data_dir = 'data/raw'\n",
    "files = sorted([f for f in os.listdir(data_dir) if f.endswith('.parquet')])\n",
    "print(f\"Checking {len(files)} parquet files for missing data...\")\n",
    "for f in files:\n",
    "    df = pd.read_parquet(os.path.join(data_dir, f))\n",
    "    if df.isnull().values.any():\n",
    "        missing_percentage = df.isnull().mean().mean() * 100\n",
    "        print(f\"Missing data found in file {f}: percentage of missing values: {missing_percentage:.2f}%\")\n",
    "    else:\n",
    "        print(f\"No missing data in file {f}.\")\n",
    "\n",
    "# Conclusion: Missing data found in 2005 (22%) and 2008 (0.03%); other years are clean.\n",
    "# Decided: start analysis from 2009 onward (see features.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01486bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 7 parquet files for differences in 'a_t', 'b_t', 'm_t'...\n",
      "No differences in file EUR_CHF_2009_S5_BA.parquet.\n",
      "No differences in file EUR_CHF_2010_S5_BA.parquet.\n",
      "No differences in file EUR_CHF_2011_S5_BA.parquet.\n",
      "No differences in file EUR_CHF_2012_S5_BA.parquet.\n",
      "No differences in file EUR_CHF_2013_S5_BA.parquet.\n",
      "No differences in file EUR_CHF_2014_S5_BA.parquet.\n",
      "No differences in file EUR_CHF_2015_S5_BA.parquet.\n"
     ]
    }
   ],
   "source": [
    "# Question: do 'a_t, 'b_t', 'm_t' (number of quotes per candle) ever differ within the same candle?\n",
    "import pandas as pd\n",
    "# Let's check all the data we have for differences\n",
    "import os\n",
    "data_dir = 'data/raw'\n",
    "files = sorted([f for f in os.listdir(data_dir) if f.endswith('.parquet')])\n",
    "print(f\"Checking {len(files)} parquet files for differences in 'a_t', 'b_t', 'm_t'...\")\n",
    "for f in files:\n",
    "    df = pd.read_parquet(os.path.join(data_dir, f))\n",
    "    diffs = df[(df['a_t'] != df['b_t']) | (df['a_t'] != df['m_t']) | (df['b_t'] != df['m_t'])]\n",
    "    if not diffs.empty:\n",
    "        print(f\"Differences found in file {f} in {len(diffs)} candles:\")\n",
    "        print(diffs)   \n",
    "    else:   \n",
    "        print(f\"No differences in file {f}.\") \n",
    "\n",
    "# Conclusion: No differences found in any year. All 'a_t', 'b_t', 'm_t' are identical per candle.\n",
    "# Decided: keep only 'm_t' and drop 'a_t', 'b_t' to save space (see features.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e709ebd",
   "metadata": {},
   "source": [
    "Test the algorithm for time-since-last-true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e054c41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a  since_a\n",
      "0   1      NaN\n",
      "1   0      1.0\n",
      "2   0      2.0\n",
      "3   0      3.0\n",
      "4   1      4.0\n",
      "5   0      1.0\n",
      "6   1      2.0\n",
      "7   0      1.0\n",
      "8   1      2.0\n",
      "9   1      1.0\n",
      "10  1      1.0\n",
      "11  0      1.0\n",
      "12  0      2.0\n",
      "13  1      3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create an artificial DataFrame for testing\n",
    "df = pd.DataFrame({\n",
    "    'a': [1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1],\n",
    "    # 'b': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "})\n",
    "\n",
    "\n",
    "def since_last_nonzero_nonshifted(series):\n",
    "    mask = series != 0\n",
    "    # cumsum increases at each nonzero, so group by this\n",
    "    group = mask.cumsum()\n",
    "    # Where group==0, it's before the first nonzero, so set to nan\n",
    "    result = series.groupby(group).cumcount() + 1\n",
    "    result[group == 0] = np.nan\n",
    "    return result.values\n",
    "\n",
    "def add_since_last_nonzero(df, col, new_col):\n",
    "    df[new_col] = since_last_nonzero_nonshifted(df[col])\n",
    "    df[new_col] = df[new_col].shift()\n",
    "    return df\n",
    "\n",
    "df = add_since_last_nonzero(df, 'a', 'since_a')\n",
    "# df = add_since_last_nonzero(df, 'b', 'since_b')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "1. **`list_instruments()`** - Lists all instruments available in your OANDA account\n",
    "2. **`get_candles_all_prices(instrument, start_date, end_date, granularity)`** - Downloads mid/bid/ask candle data for a single instrument\n",
    "\n",
    "### Key Features:\n",
    "- Handles OANDA's 5000 candle limit per request with automatic pagination\n",
    "- Rate limiting protection with delays between requests\n",
    "- Supports multiple granularities (daily, hourly, minute-level, etc.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
